# Egocentric Audio-Visual Object Localization 
This is the PyTorch implementation of the paper "Egocentric Audio-Visual Object Localization."
 
## Overview
<p align="center">
 <img align="center" src="./fig/motivation.png" width=70%>
</p>

We explore the task of egocentric audio-visual object localization, which aims to localize objects that emit sounds in the first-person recordings. In this work, we propose a new framework to address the uniqueness of egocentric videos by answering the following two questions: (1) how to associate visual content with audio representations while out-of-view sounds may exist; (2) how to persistently associate audio features with visual content that are captured under different viewpoints.

## Epic Sounding Object dataset

Comming soon

## Citation
If you find our work useful for your research, please consider cite our paper. :smile:
